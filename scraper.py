import asyncio
from telegram_bot.telegram_bot import TelegramPriceBot
from urllib.parse import quote_plus
import time
import random
import threading
import signal
import sys
import os
from dotenv import load_dotenv

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.firefox import GeckoDriverManager

from sqlalchemy import create_engine, Table, Column, Integer, String, Numeric, ForeignKey, MetaData, select, Boolean, DateTime
from datetime import datetime, timedelta

load_dotenv()

DATABASE_URL = os.getenv('DATABASE_URL')

engine = create_engine(DATABASE_URL, echo=False)
metadata = MetaData()

products = Table("products", metadata,
    Column("id", Integer, primary_key=True),
    Column("name", String, nullable=False),
    Column("website", String, nullable=False),
    Column("category", String, nullable=False),
    Column("product_link", String),
)

prices = Table("prices", metadata,
    Column("id", Integer, primary_key=True),
    Column("product_id", Integer, ForeignKey("products.id"), nullable=False),
    Column("price", Numeric, nullable=False),
    Column("collected_at", DateTime, default=datetime.now),
    Column("last_checked_at", DateTime, default=datetime.now),
    Column("price_changed_at", DateTime, default=datetime.now),
    Column("check_count", Integer, default=1),
)

search_configs = Table("search_configs", metadata,
    Column("id", Integer, primary_key=True),
    Column("search_text", String, nullable=False),
    Column("category", String, nullable=False),
    Column("website", String, nullable=False),
    Column("is_active", Boolean, default=True),
    Column("created_at", DateTime, default=datetime.now),
)

keyword_groups = Table("keyword_groups", metadata,
    Column("id", Integer, primary_key=True),
    Column("search_config_id", Integer, ForeignKey("search_configs.id", ondelete="CASCADE"), nullable=False),
    Column("keywords", String, nullable=False),  # Comma-separated keywords within a group
    Column("created_at", DateTime, default=datetime.now),
)

gecko_path = GeckoDriverManager().install()

TIMEOUT = 25

# Global stop event
stop_event = threading.Event()

def random_delay():
    # Check for stop event during delays
    delay_time = random.uniform(1, 3)
    if stop_event.wait(delay_time):
        return True  # Stop requested
    return False

def normalize_price_pichau(price_text):
    """
    Fun√ß√£o para normalizar pre√ßos da Pichau e evitar problemas de formata√ß√£o
    """
    try:
        # Remove espa√ßos e caracteres especiais
        clean_text = price_text.replace("\xa0", " ").replace("R$", "").replace(" ", "").strip()
        
        # Se tem ponto E v√≠rgula, assume formato brasileiro correto (ex: 1.499,99)
        if "." in clean_text and "," in clean_text:
            # Remove pontos (separadores de milhares) e troca v√≠rgula por ponto
            clean_text = clean_text.replace(".", "").replace(",", ".")
            price = float(clean_text)
        # Se tem apenas v√≠rgula, assume formato brasileiro (ex: 499,99)
        elif "," in clean_text and "." not in clean_text:
            clean_text = clean_text.replace(",", ".")
            price = float(clean_text)
        # Se tem apenas ponto, verifica se √© separador decimal ou milhares
        elif "." in clean_text and "," not in clean_text:
            # Se tem mais de 3 d√≠gitos ap√≥s o ponto, provavelmente √© erro de formata√ß√£o
            parts = clean_text.split(".")
            if len(parts) == 2 and len(parts[1]) > 2:
                # Exemplo: 49999.00 deve ser 499.99
                # Reconstr√≥i o pre√ßo dividindo por 100
                price = float(clean_text) / 100
            else:
                # Formato normal com ponto como decimal
                price = float(clean_text)
        else:
            # Apenas n√∫meros
            price = float(clean_text)
        
        # Valida√ß√£o adicional: se o pre√ßo √© muito alto (>10000), pode ser erro
        if price > 10000:
            # Tenta dividir por 100 para corrigir
            corrected_price = price / 100
            # Se o pre√ßo corrigido fica entre 10 e 10000, provavelmente estava errado
            if 10 <= corrected_price <= 10000:
                print(f"‚ö†Ô∏è Pre√ßo suspeito corrigido: {price} -> {corrected_price}")
                price = corrected_price
        
        return price
        
    except Exception as e:
        print(f"‚ùå Erro ao normalizar pre√ßo '{price_text}': {e}")
        return 0.0

def calculate_weighted_average(product_id):
    """
    Calcula a m√©dia hist√≥rica ponderada EXCLUINDO o pre√ßo atual
    Segue EXATAMENTE a mesma l√≥gica do frontend
    """
    try:
        with engine.begin() as conn:
            # Buscar TODO o hist√≥rico de pre√ßos para este produto, ordenado por data
            query = select(
                prices.c.price,
                prices.c.check_count,
                prices.c.price_changed_at
            ).where(
                prices.c.product_id == product_id
            ).order_by(prices.c.price_changed_at.desc())
            
            all_prices = conn.execute(query).fetchall()
            
            if len(all_prices) <= 1:
                # Se s√≥ tem o pre√ßo atual ou nenhum, n√£o h√° hist√≥rico para calcular
                return None
            
            # O primeiro √© o pre√ßo atual, pegamos os hist√≥ricos (excluindo o atual)
            historical_prices = all_prices[1:]  # Remove o pre√ßo atual
            
            if not historical_prices:
                return None
            
            # Calcular m√©dia ponderada usando check_count como peso
            total_weight = sum(max(1, p.check_count or 1) for p in historical_prices)
            weighted_sum = sum(float(p.price) * max(1, p.check_count or 1) for p in historical_prices)
            
            if total_weight > 0:
                return weighted_sum / total_weight
            else:
                return None
                
    except Exception as e:
        print(f"‚ùå Erro ao calcular m√©dia hist√≥rica: {e}")
        return None

def check_promotion_and_notify(product_id, product_name, current_price, website):
    """
    Verifica se √© uma promo√ß√£o real (>10% desconto vs m√©dia hist√≥rica) e notifica
    """
    try:
        weighted_average = calculate_weighted_average(product_id)
        
        if not weighted_average or weighted_average == current_price:
            print(f"üìä {product_name}: Sem hist√≥rico suficiente para calcular promo√ß√£o")
            return False
        
        # Calcular desconto exatamente como no frontend
        discount_percent = ((weighted_average - current_price) / weighted_average) * 100
        
        print(f"üìä {product_name}: Pre√ßo atual R$ {current_price:.2f} vs M√©dia hist√≥rica R$ {weighted_average:.2f}")
        print(f"üìä Desconto calculado: {discount_percent:.1f}%")
        
        # Crit√©rios para notifica√ß√£o (mesmo do frontend)
        is_significant_discount = discount_percent >= 10  # M√≠nimo 10% para notifica√ß√£o
        has_minimum_price = current_price >= 20  # Pre√ßo m√≠nimo R$ 20
        is_reasonable_discount = discount_percent <= 80  # M√°ximo 80% (evita erros)
        discount_amount = weighted_average - current_price
        
        is_promotion = (is_significant_discount and 
                       has_minimum_price and 
                       is_reasonable_discount and 
                       discount_amount > 0)
        
        if is_promotion:
            print(f"üî• PROMO√á√ÉO DETECTADA! {product_name} - {discount_percent:.1f}% desconto")
            
            # Enviar notifica√ß√£o do Telegram
            try:
                bot = TelegramPriceBot()
                message = f"üî• PROMO√á√ÉO REAL DETECTADA!\n\n"
                message += f"üì± {product_name}\n"
                message += f"üè™ {website.upper()}\n\n"
                message += f"üí∞ Pre√ßo atual: R$ {current_price:.2f}\n"
                message += f"üìä M√©dia hist√≥rica: R$ {weighted_average:.2f}\n"
                message += f"üìâ Desconto: {discount_percent:.1f}%\n"
                message += f"üíµ Economia: R$ {discount_amount:.2f}"
                
                asyncio.run(bot.send_message(message))
                print(f"‚úÖ Notifica√ß√£o de promo√ß√£o enviada!")
                return True
            except Exception as e:
                print(f"‚ùå Erro ao enviar notifica√ß√£o de promo√ß√£o: {e}")
        else:
            reasons = []
            if not is_significant_discount:
                reasons.append(f"desconto insuficiente ({discount_percent:.1f}% < 10%)")
            if not has_minimum_price:
                reasons.append(f"pre√ßo muito baixo (R$ {current_price:.2f} < R$ 20)")
            if not is_reasonable_discount:
                reasons.append(f"desconto suspeito ({discount_percent:.1f}% > 80%)")
            if discount_amount <= 0:
                reasons.append("pre√ßo atual >= m√©dia")
            
            print(f"‚ö™ N√£o √© promo√ß√£o: {', '.join(reasons)}")
        
        return False
        
    except Exception as e:
        print(f"‚ùå Erro ao verificar promo√ß√£o: {e}")
        return False

def save_product(name, price, website, category, product_link, keywords_matched=None):
    if price > 10.0:
        print(f"\nüíæ Tentando salvar: {name} | {website} | {category} | {price} | Matched: {keywords_matched}")

        try:
            with engine.begin() as conn:
                # 1. Verificar se produto existe, se n√£o, criar
                query = select(products.c.id).where(
                    products.c.name == name,
                    products.c.website == website
                )
                product_id = conn.execute(query).scalar()
                
                if product_id is None:
                    result = conn.execute(products.insert().values(
                        name=name,
                        website=website,
                        category=category,
                        product_link=product_link
                    ))
                    product_id = result.inserted_primary_key[0]
                    print(f"üëâ Produto novo inserido com id={product_id}")
                
                # 2. Buscar √∫ltimo pre√ßo registrado para este produto
                last_price_query = select(
                    prices.c.price,
                    prices.c.check_count,
                    prices.c.id,
                    prices.c.last_checked_at
                ).where(
                    prices.c.product_id == product_id
                ).order_by(
                    prices.c.last_checked_at.desc()
                ).limit(1)
                
                last_price_result = conn.execute(last_price_query).first()
                
                current_time = datetime.now()
                
                # 3. Decidir se inserir novo registro ou atualizar existente
                if last_price_result is None:
                    # Primeiro pre√ßo para este produto
                    conn.execute(prices.insert().values(
                        product_id=product_id,
                        price=price,
                        collected_at=current_time,
                        last_checked_at=current_time,
                        price_changed_at=current_time,
                        check_count=1
                    ))
                    print(f"‚úÖ Primeiro pre√ßo inserido: R$ {price}")
                    
                else:
                    # Converter pre√ßos para float para compara√ß√£o segura
                    last_price = float(last_price_result.price)
                    current_price = float(price)
                    
                    # Verificar se o √∫ltimo registro √© da busca atual (√∫ltimos 5 minutos)
                    last_checked = last_price_result.last_checked_at
                    if last_checked.tzinfo is not None:
                        last_checked = last_checked.replace(tzinfo=None)
                    time_diff = current_time - last_checked
                    is_same_search = time_diff.total_seconds() < 300  # 5 minutos
                    
                    if abs(last_price - current_price) > 0.01:  # Mudan√ßa significativa (> R$ 0,01)
                        # Pre√ßo mudou - inserir novo registro
                        conn.execute(prices.insert().values(
                            product_id=product_id,
                            price=current_price,
                            collected_at=current_time,
                            last_checked_at=current_time,
                            price_changed_at=current_time,
                            check_count=1
                        ))
                        price_diff = current_price - last_price
                        percentage = (price_diff / last_price) * 100
                        print(f"üìà Pre√ßo mudou: R$ {last_price} ‚Üí R$ {current_price} ({percentage:+.1f}%)")
                        
                        # üö® NOVA L√ìGICA: S√≥ verifica promo√ß√£o se N√ÉO for da mesma busca
                        if not is_same_search:
                            check_promotion_and_notify(product_id, name, current_price, website)
                        else:
                            print(f"üí° Mudan√ßa na mesma busca - verifica√ß√£o de promo√ß√£o ignorada")
                        
                    else:
                        # Pre√ßo igual - apenas atualizar last_checked_at e incrementar contador
                        current_check_count = last_price_result.check_count or 0
                        new_check_count = current_check_count + 1
                        
                        conn.execute(
                            prices.update()
                            .where(prices.c.id == last_price_result.id)
                            .values(
                                last_checked_at=current_time,
                                check_count=new_check_count
                            )
                        )
                        print(f"üîÑ Pre√ßo mantido R$ {current_price} (verifica√ß√£o #{new_check_count})")

        except Exception as e:
            print(f"üî• Erro inesperado no save_product: {e}")
            import traceback
            traceback.print_exc()

def get_search_configs_with_keywords():
    """Get all active search configurations with their keyword groups"""
    try:
        with engine.begin() as conn:
            # Get active search configs
            configs_query = select(
                search_configs.c.id,
                search_configs.c.search_text,
                search_configs.c.category,
                search_configs.c.website
            ).where(search_configs.c.is_active == True)
            
            configs = conn.execute(configs_query).fetchall()
            
            # For each config, get its keyword groups
            configs_with_keywords = []
            for config in configs:
                keywords_query = select(keyword_groups.c.keywords).where(
                    keyword_groups.c.search_config_id == config.id
                )
                keyword_rows = conn.execute(keywords_query).fetchall()
                
                # Convert keyword groups to the expected format
                keyword_groups_list = []
                for row in keyword_rows:
                    # Split comma-separated keywords and clean them
                    keywords_in_group = [k.strip() for k in row.keywords.split(',') if k.strip()]
                    if keywords_in_group:
                        keyword_groups_list.append(keywords_in_group)
                
                if keyword_groups_list:  # Only include configs that have keywords
                    configs_with_keywords.append({
                        "search_text": config.search_text,
                        "keywords": keyword_groups_list,
                        "category": config.category,
                        "website": config.website
                    })
            
            return configs_with_keywords
    except Exception as e:
        print(f"üî• Error fetching search configurations: {e}")
        return []

def scrape_kabum(driver, wait, query, wordlist, category):
    if stop_event.is_set():
        return
        
    print(f"\nüîç Searching on Kabum: {query}")
    base_url = "https://www.kabum.com.br"
    url = f"{base_url}/busca/{quote_plus(query.replace(' ', '-'))}?page_number=1&page_size=100&facet_filters=&sort=most_searched&variant=null&redirect_terms=true"
    
    try:
        driver.get(url)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "article.productCard")))
    except:
        print(f"‚ö†Ô∏è Page didn't load or doesn't exist")
        return
        
    if stop_event.is_set():
        return
        
    soup = BeautifulSoup(driver.page_source, "html.parser")
    cards = soup.select("article.productCard")
    
    for card in cards:
        if stop_event.is_set():
            return
            
        try:
            base_name = card.select_one(".nameCard").get_text(strip=True).lower()
            matched_keywords = []
            for words in wordlist:
                if all(p.lower() in base_name for p in words):
                    matched_keywords.append(words)
            
            if matched_keywords:
                product_link_elem = card.select_one("a.productLink")
                product_link = base_url + product_link_elem.get("href")
                
                # Extrair o ID do produto da classe data-smarthintproductid (est√° no pr√≥prio elemento <a>)
                product_id = product_link_elem.get("data-smarthintproductid")
                
                # Determinar o nome final do produto
                if product_id:
                    final_name = f"{base_name} #{product_id}"
                    
                    # Verificar se existe um produto com o nome base sem ID
                    import re
                    with engine.begin() as conn:
                        # Buscar produto existente com nome base
                        existing_query = select(products.c.id, products.c.name).where(
                            products.c.name == base_name,
                            products.c.website == "kabum"
                        )
                        existing_product = conn.execute(existing_query).first()
                        
                        if existing_product:
                            # Verificar se o nome N√ÉO termina com # seguido de n√∫meros (qualquer quantidade)
                            existing_name = existing_product.name
                            if not re.search(r'#\d+$', existing_name):
                                # Produto existe sem ID, vamos atualizar o nome
                                conn.execute(
                                    products.update()
                                    .where(products.c.id == existing_product.id)
                                    .values(name=final_name)
                                )
                                print(f"üîÑ Nome atualizado: '{existing_name}' ‚Üí '{final_name}'")
                else:
                    final_name = base_name
                
                price_elem = card.select_one('[data-testid="price-value"], .priceCard')
                price_text = price_elem.get_text(strip=True)
                price = float(price_text.split("R$")[1].replace(".", "").replace(",", "."))
                save_product(final_name, price, "kabum", category, product_link, matched_keywords)
                
        except Exception as e:
            print(f"‚ùå Erro no parsing Kabum: {e}")

def scrape_pichau(driver, wait, query, wordlist, category):
    if stop_event.is_set():
        return
        
    print(f"\nüîç Searching on Pichau: {query}")
    base_url = "https://www.pichau.com.br"
    url = f"{base_url}{query}"
    
    try:
        driver.get(url)
        # Aguarda um pouco mais para garantir que os pre√ßos carregem completamente
        time.sleep(3)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-cy='list-product']")))
        # Aguarda adicional para garantir que os pre√ßos estejam formatados corretamente
        time.sleep(2)
    except:
        print(f"‚ö†Ô∏è Page didn't load or doesn't exist")
        return
        
    if stop_event.is_set():
        return
        
    soup = BeautifulSoup(driver.page_source, "html.parser")
    cards = soup.select("[data-cy='list-product']")
    
    for card in cards:
        if stop_event.is_set():
            return
            
        try:
            name = card.select_one("h2").get_text(strip=True).lower()
            matched_keywords = []
            for words in wordlist:
                if all(p.lower() in name for p in words):
                    matched_keywords.append(words)
            
            if matched_keywords:
                product_link = base_url + card.get("href")
                
                # Tenta diferentes seletores para o pre√ßo
                price_elem = card.select_one("div.mui-12athy2-price_vista, .price, [data-testid='price']")
                if not price_elem:
                    print(f"‚ö†Ô∏è Pre√ßo n√£o encontrado para: {name}")
                    continue
                    
                price_text = price_elem.get_text(strip=True).replace("\xa0", " ")
                
                # Usa a fun√ß√£o de normaliza√ß√£o espec√≠fica para Pichau
                price = normalize_price_pichau(price_text)
                
                if price > 0:
                    save_product(name, price, "pichau", category, product_link, matched_keywords)
                else:
                    print(f"‚ö†Ô∏è Pre√ßo inv√°lido para {name}: {price_text}")
                
        except Exception as e:
            print(f"‚ùå Erro no parsing Pichau: {e}")

def scrape_terabyte(driver, wait, query, wordlist, category):
    if stop_event.is_set():
        return
        
    print(f"\nüîç Searching on Terabyte: {query}")
    base_url = "https://www.terabyteshop.com.br"
    url = f"{base_url}/busca?str={quote_plus(query)}"
    
    try:
        driver.get(url)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".product-item")))
    except:
        print(f"‚ö†Ô∏è Page didn't load or doesn't exist")
        return
        
    if stop_event.is_set():
        return
        
    soup = BeautifulSoup(driver.page_source, "html.parser")
    cards = soup.select(".product-item")
    
    for card in cards:
        if stop_event.is_set():
            return
            
        try:
            name = card.select_one("h2").get_text(strip=True).lower()
            matched_keywords = []
            for words in wordlist:
                if all(p.lower() in name for p in words):
                    matched_keywords.append(words)
            
            if matched_keywords:
                product_link = card.select_one("a.product-item__image").get("href")
                price_elem = card.select_one(".product-item__new-price span")
                price_text = price_elem.get_text(strip=True)
                price = float(price_text.replace("R$", "").replace(".", "").replace(",", ".").strip())
                save_product(name, price, "terabyteshop", category, product_link, matched_keywords)
                
        except Exception as e:
            print(f"‚ùå Erro no parsing Terabyte: {e}")

def create_driver():
    opts = Options()
    opts.set_preference("permissions.default.image", 2)
    opts.set_preference("dom.ipc.plugins.enabled.libflashplayer.so", "false")
    opts.set_preference("media.autoplay.default", 5)
    opts.add_argument("--headless")
    
    # Rotate user agents
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15"
    ]
    opts.set_preference("general.useragent.override", random.choice(user_agents))
    
    # Disable WebDriver flag
    opts.set_preference("dom.webdriver.enabled", False)
    opts.set_preference("useAutomationExtension", False)
    
    driver = webdriver.Firefox(service=Service(gecko_path), options=opts)
    
    # Execute CDP command to hide selenium detection
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def start_search():
    def search_task():
        try:
            while not stop_event.is_set():
                print("\n=== Starting new scan ===")
                start_time = time.time()
                
                if stop_event.is_set():
                    break
                
                try:
                    # Get all search configurations with their keywords
                    all_searches = get_search_configs_with_keywords()
                    
                    if not all_searches:
                        print("‚ö†Ô∏è No active search configurations found")
                        elapsed = time.time() - start_time
                        delay = max(360 - elapsed, 60)
                        print(f"\n‚è≥ Next scan in {delay//60} minutes...")
                        
                        # Wait with stop event checking
                        if stop_event.wait(delay):
                            break
                        continue
                    
                    # Organize searches by website
                    searches_by_website = {
                        "kabum": [],
                        "pichau": [],
                        "terabyte": []
                    }
                    
                    for search in all_searches:
                        website = search['website']
                        if website in searches_by_website:
                            searches_by_website[website].append(search)
                    
                    # Function to process all searches for one website
                    def process_website(website_name, searches):
                        for search in searches:
                            if stop_event.is_set():
                                return
                                
                            driver = None
                            try:
                                driver = create_driver()
                                wait = WebDriverWait(driver, TIMEOUT)
                                
                                if website_name == "kabum":
                                    scrape_kabum(driver, wait, search["search_text"], 
                                                search["keywords"], search["category"])
                                elif website_name == "pichau":
                                    scrape_pichau(driver, wait, search["search_text"], 
                                                 search["keywords"], search["category"])
                                elif website_name == "terabyte":
                                    scrape_terabyte(driver, wait, search["search_text"], 
                                                   search["keywords"], search["category"])
                                
                                # Check for stop before delay
                                if stop_event.is_set():
                                    return
                                    
                                # Use interruptible delay
                                if random_delay():
                                    return
                                    
                            except Exception as e:
                                print(f"üî• Error during {website_name} search: {e}")
                            finally:
                                if driver:
                                    try:
                                        driver.quit()
                                    except:
                                        pass
                    
                    # Create and start threads (max 3 - one per website)
                    threads = []
                    for website, searches in searches_by_website.items():
                        if searches and not stop_event.is_set():  # Only create thread if there are searches for this website
                            t = threading.Thread(
                                target=process_website,
                                args=(website, searches),
                                daemon=True  # Make threads daemon so they don't prevent shutdown
                            )
                            threads.append(t)
                            t.start()
                            
                            # Stagger thread starts with stop checking
                            if stop_event.wait(random.uniform(0.5, 1.5)):
                                break
                    
                    # Wait for all threads to complete or stop event
                    for t in threads:
                        while t.is_alive() and not stop_event.is_set():
                            t.join(timeout=1)  # Check every second
                        
                        if stop_event.is_set():
                            break
                
                except Exception as e:
                    print(f"üî• Error in search cycle: {e}")
                
                if stop_event.is_set():
                    break
                    
                elapsed = time.time() - start_time
                delay = max(360 - elapsed, 60)
                print(f"\n‚è≥ Next scan in {delay//60} minutes...")
                
                # Wait with stop event checking
                if stop_event.wait(delay):
                    break
                    
        except Exception as e:
            print(f"Unexpected error in search task: {e}")
        finally:
            print("üõë Search task stopped")

    search_thread = threading.Thread(target=search_task, daemon=True)
    search_thread.start()
    return search_thread

def signal_handler(sig, frame):
    print("\nüõë Received shutdown signal, stopping gracefully...")
    stop_event.set()

if __name__ == "__main__":
    # Set up signal handling
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    print("üöÄ Starting PC Scraper...")
    print("Press Ctrl+C to stop gracefully")
    
    search_thread = start_search()
    
    try:
        # Keep main thread alive and check for stop event
        while not stop_event.is_set():
            time.sleep(0.5)
    except KeyboardInterrupt:
        print("\nüõë Keyboard interrupt received")
        stop_event.set()
    finally:
        print("‚è≥ Waiting for threads to finish...")
        
        # Give threads time to cleanup
        if search_thread.is_alive():
            search_thread.join(timeout=10)  # Wait max 10 seconds
        
        print("‚úÖ Clean shutdown complete.")
        sys.exit(0)